# 关于环境搭建的一些问题

在进行spark作业提交时遇到的一些各种各样的问题，最终排查后确定为虚拟机配置太低，导致一系列其他的问题出现；所以进行了环境的重新搭建。目前环境工作正常。

## 1.目前的环境配置

- 虚拟机：Ubuntu16.04_x86_64架构，4核，4G运行内存，磁盘空间30G；

目前这个虚拟机配置是能够支持整个开发生态正常运行的，包括airflow-spark、postgres、spark-master和3个spark-worker同时启动；能够正常进行作业提交运行。也终于能够进入到spark作业的开发学习和实践了。

## 2.问题整理

### 2.1配置不足导致的一些问题

之前由于虚拟机只有2核2G，不能够完全保证我们的整个开发环境的正常运行；所以在进行作业是会遇到一些始料未及的问题；现做整理如下：

#### 2.1.1scheduler掉线

偶尔会出现scheduler掉线的情况，即使任务启动也没法儿正常调度，每次都要重启，很烦；

#### 2.1.2任务卡顿甚至fail，页面刷新慢

在普通helloworld任务运行时都会出现卡顿的情况，页面无法刷新；甚至出现任务直接**fail**掉的情况，日志给出的错误基本没法儿让人往配置不够想。时而能成功，时而不能成功，让人摸不着头脑；在运行数据库测试的demo时airflow根本无法正常完成调度工作，经常卡死，日志常见报错如下：**Cannot execute{xxxxx,xxxx}**；让人想不通，这本来是能够正常工作的脚本，为什么会出现无法执行的报错。后来仔细翻看日志才发现，在连接worker进行任务调度时，worker经常会出现丢失（worker lost）的情况，job还没分配，worker就掉线了，整个任务自然是无法正常进行了。

这个恶心的问题卡了半天，后来想了想，是不是因为虚拟机配置拉跨，才导致，调度器一启动worker就掉线；然后我就尝试只开两个worker，发现helloworld能够跑通了，但是还是慢，而且数据连接测试的任务仍然不能工作，日志又给出了其他的错误。

搞了半天，还是放弃了决定重新配置开发环境。

### 2.2环境搭建的一些坑

#### 2.2.1docker支持的系统架构

docker只支持AMD64，x86_64这些架构，官网有给出；下载系统镜像的时候要注意，我下了个i686装docker，出现了一堆问题，怎么都解决不了

**docker安装问题：E: Package 'docker-ce' has no installation candidate**

像上面这个报错，网上很多解决方案，但没有一个人提到会因为是docker在不支持的系统架构安装才会出现的问题。也是卡了我很久，才发现不同的系统架构在安装docker时的一些配置略有不同，最终确定了docker不支持我这个i686的系统。

#### 2.2.2权限问题

我在进行环境搭建的时候用的root账户，后边用普通在其他主机访问的时候没法儿修改文件；又花了时间去修改权限，Linux操作不熟悉真的拉跨；

所以在搭建环境下载文件的时候记得用普通用户登陆，或者直接给你要远程登陆的用户root权限；不然又出现一些与主要矛盾不相干的问题，烦死人了。

## 3环境搭建

不管怎么说，花了一天时间算是搭出来了一个能够正常工作的环境，简单过一下过程：

- ```
  - 1.docker：官网给出的使用手册，一步步走就行；
  - 2.docker-compose：也是官网手册，不踩雷，按部就班没有问题；
  - 3.按照老师给的博客教程把项目git下来
  - 4.通过项目提供的Dockerfile构建airflow-spark镜像，只需要自己构建这个镜像就可以了；因为按项目jupyter的Dockerfile构建需要的一些东西已经更新或者过时，没法儿正常构建他给出的jupyter镜像了，而其他的spark和postgres镜像直接拉取现有的版本就行。
  - 5.在airflow/docker/文件夹下的docker-compose.yml文件里去掉jupyter构建的那部分配置，其他的worker可以自己增减
  - 6.docker-compose up命令运行起来就可以了
  ```

